{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "addf8ba1-4ab5-485d-83fa-69750ca7fc57",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7cf20e-1c27-4539-be62-c3a8e31e07f4",
   "metadata": {},
   "source": [
    "## Load mosquito data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135c8fe-f2ab-4447-a8c1-97a4f479bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display option to show all columns\n",
    "#pd.set_option('display.max_columns', None)\n",
    "# Read the txt file into a pandas DataFrame\n",
    "df = pd.read_csv('data/raw_data/West_Nile_Virus__WNV__Mosquito_Test_Results.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779f0de-e9fb-4b7f-a9b3-2285cbbf5c4e",
   "metadata": {},
   "source": [
    "## identify missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595e29c-fd6f-4416-b69d-ea8306e52adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_count = data.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f3df2-7bb8-422a-b1cf-b1bccf07f0fd",
   "metadata": {},
   "source": [
    "## find the missing lat/long and group them by block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d6eed-0a84-4b14-9eee-084aca12481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_lat_long_data = data[data['LATITUDE'].isnull() | data['LONGITUDE'].isnull()]\n",
    "\n",
    "# Group by 'BLOCK', count the missing values and list the 'TEST DATE' for each\n",
    "block_missing_summary = missing_lat_long_data.groupby('BLOCK').agg(\n",
    "    Missing_Count=pd.NamedAgg(column='LATITUDE', aggfunc='size'), # Count of missing values\n",
    "    Test_Dates=pd.NamedAgg(column='TEST DATE', aggfunc=lambda x: x.unique().tolist()) # Unique test dates\n",
    ").reset_index()\n",
    "\n",
    "# Display the summary\n",
    "block_missing_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5a0cf8-24f8-49b4-adb9-b94c7512de11",
   "metadata": {},
   "source": [
    "### This code was used to check to see if there were any lat/long listed for at least one of the block records. To check if this code works, I added fake data in two fields. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d408c-70c7-4c25-b7ba-4ab0e147768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the list of blocks with missing lat/long data\n",
    "blocks_with_missing_values = block_missing_summary['BLOCK'].unique()\n",
    "\n",
    "# Filter the original dataset for entries that are in the list of blocks with missing values\n",
    "# but have valid latitude and longitude data\n",
    "blocks_with_valid_lat_long = data[\n",
    "    data['BLOCK'].isin(blocks_with_missing_values) &\n",
    "    data['LATITUDE'].notnull() &\n",
    "    data['LONGITUDE'].notnull()\n",
    "]\n",
    "\n",
    "# Group by 'BLOCK' and list the unique latitude and longitude values for these entries\n",
    "block_valid_lat_long_summary = blocks_with_valid_lat_long.groupby('BLOCK').agg({\n",
    "    'LATITUDE': lambda x: x.unique().tolist(),\n",
    "    'LONGITUDE': lambda x: x.unique().tolist()\n",
    "}).reset_index()\n",
    "\n",
    "# The resulting DataFrame 'block_valid_lat_long_summary' will contain each block along with\n",
    "# the associated valid latitude and longitude values that exist in the dataset.\n",
    "block_valid_lat_long_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6866791-93aa-4974-a735-b37db3ef4ac7",
   "metadata": {},
   "source": [
    "### Because I am able to get the lat and long for these blocks, I will keep them and dropp all the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289675d5-2bc5-418d-b1b4-cde072815e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the blocks to keep even if they have missing lat/long\n",
    "blocks_to_keep = ['100XX W OHARE', '100XX W OHARE AIRPORT', '4XX W 127TH']\n",
    "\n",
    "# Filter the data to exclude rows with missing lat/long unless the block contains one of the specified blocks to keep\n",
    "data_filtered = data[\n",
    "    (~data['LATITUDE'].isnull() & ~data['LONGITUDE'].isnull()) |  # Keep rows with valid lat/long\n",
    "    (data['BLOCK'].str.contains('|'.join(blocks_to_keep)))  # Or rows that contain the specified blocks\n",
    "]\n",
    "\n",
    "# The resulting DataFrame 'data_filtered' will have the rows with missing values dropped,\n",
    "# except for the specified blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911071e-5cc1-4b2a-bed0-4bfeea60b211",
   "metadata": {},
   "source": [
    "Adding Lat and long for the 3 blocks and saving new dataset to the processed data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb10df-f423-4bc1-9de1-c2364150fafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your dataframe is named data_filtered\n",
    "# Update the lat/long values for the specified locations\n",
    "data_filtered.loc[data_filtered['BLOCK'] == '100XX W OHARE', ['LATITUDE', 'LONGITUDE']] = 41.978611, -87.904724\n",
    "data_filtered.loc[data_filtered['BLOCK'] == '100XX W OHARE AIRPORT', ['LATITUDE', 'LONGITUDE']] = 41.978611, -87.904724\n",
    "data_filtered.loc[data_filtered['BLOCK'] == '4XX W 127TH', ['LATITUDE', 'LONGITUDE']] = 41.66318849, -87.63267836\n",
    "\n",
    "# Save the updated dataframe to a new CSV file in the same directory\n",
    "data_filtered.to_csv('data/processed_data/wnv_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ab58a-85b8-4ac0-b783-e07918a92f34",
   "metadata": {},
   "source": [
    "## Load weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025fd827-b701-46e0-8dac-217f90c17011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set the display option to show all columns\n",
    "#pd.set_option('display.max_columns', None)\n",
    "# Read the txt file into a pandas DataFrame\n",
    "wx = pd.read_csv('data/raw_data/ORD_weather.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "wx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4a180-1123-4d6d-90cc-230c9c04b63d",
   "metadata": {},
   "source": [
    "### identify missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71903747-cb05-42ee-88be-81bc8e016704",
   "metadata": {},
   "source": [
    "There are no missing values so we can proceed with the next step. date ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8a48dc-59ab-4b29-aba7-13c4085f48f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#date range for the mosquito dataset\n",
    "year_min_mos = df['SEASON YEAR'].min()\n",
    "year_max_mos = df['SEASON YEAR'].max()\n",
    "\n",
    "#date range for the weather datasetab\n",
    "year_min_wx = wx['DATE'].min()\n",
    "year_max_wx = wx['DATE'].max()\n",
    "\n",
    "print(f\"The range of the mosquito dataset is {year_min_mos} to {year_max_mos}\")\n",
    "print(f\"The range of the mosquito dataset is {year_min_wx} to {year_max_wx}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840cb8ae-ebe1-4b20-baea-34d9c1a067ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected file path\n",
    "file_path = 'data/processed_data/wnv_cleaned.csv'\n",
    "\n",
    "# Read the CSV file\n",
    "mos_clean = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "mos_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1014e-c9cf-42a0-bf85-78a1aa3efe5b",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba50e9-655b-4f54-89a6-0faf7bc748be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to keep only the records where 'SEASON YEAR' is >= 2018\n",
    "mos_trim = mos_clean[mos_clean['SEASON YEAR'] >= 2018]\n",
    "\n",
    "# Find the minimum value of 'SEASON YEAR' in the filtered DataFrame\n",
    "min_season_year = mos_trim['SEASON YEAR'].min()\n",
    "\n",
    "min_season_year\n",
    "\n",
    "mos_trim.to_csv('data/processed_data/wnv_trim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f356b3fa-6ac0-45bc-b020-7890679cfeb3",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week 39 (09/24/12 –09/28/12)</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Collections (Number of Traps X Frequency...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number of Female Culex Trapped and Tested</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number of Pools Tested</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number of Pools Positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number of Community Areas with Positive Mosqui...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cumulative Totals</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Total Collections (Number of Traps X Frequency...</td>\n",
       "      <td>2,540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Number of Female Culex Trapped and Tested</td>\n",
       "      <td>36,021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Number of Pools Tested</td>\n",
       "      <td>2,478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Number of Pools Positive</td>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Number of Community Areas with Positive Mosqui...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Week 39 (09/24/12 –09/28/12)        \n",
       "0   Total Collections (Number of Traps X Frequency...     150\n",
       "1           Number of Female Culex Trapped and Tested     567\n",
       "2                              Number of Pools Tested     109\n",
       "3                            Number of Pools Positive       1\n",
       "4   Number of Community Areas with Positive Mosqui...       1\n",
       "5                                   Cumulative Totals        \n",
       "6   Total Collections (Number of Traps X Frequency...   2,540\n",
       "7           Number of Female Culex Trapped and Tested  36,021\n",
       "8                              Number of Pools Tested   2,478\n",
       "9                            Number of Pools Positive     437\n",
       "10  Number of Community Areas with Positive Mosqui...      58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = '../data/raw_data/ArboReport10012012 (1).pdf'\n",
    "\n",
    "tables = []  # To store all the tables extracted\n",
    "\n",
    "# Open the PDF file\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    page = pdf.pages[0]  # Assuming the table is on the first page\n",
    "    table = page.extract_table()  # Extract the table\n",
    "    if table:  # If a table is found\n",
    "        tables.append(pd.DataFrame(table[1:], columns=table[0]))\n",
    "\n",
    "# Display the first table as a DataFrame\n",
    "if tables:\n",
    "    display(tables[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3132c8d2-d329-407f-ae18-dc39d18b5e27",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.10.4-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting pdfminer.six==20221105 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting Pillow>=9.1 (from pdfplumber)\n",
      "  Downloading pillow-10.2.0-cp311-cp311-macosx_10_10_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.27.0-py3-none-macosx_10_13_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /Users/andrewruiz/anaconda3/envs/HPAM9000/lib/python3.11/site-packages (from pdfminer.six==20221105->pdfplumber) (3.3.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20221105->pdfplumber)\n",
      "  Downloading cryptography-42.0.5-cp39-abi3-macosx_10_12_universal2.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/andrewruiz/anaconda3/envs/HPAM9000/lib/python3.11/site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /Users/andrewruiz/anaconda3/envs/HPAM9000/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
      "Downloading pdfplumber-0.10.4-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-10.2.0-cp311-cp311-macosx_10_10_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.27.0-py3-none-macosx_10_13_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading cryptography-42.0.5-cp39-abi3-macosx_10_12_universal2.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, Pillow, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed Pillow-10.2.0 cryptography-42.0.5 pdfminer.six-20221105 pdfplumber-0.10.4 pypdfium2-4.27.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74cd1f-f7b4-42b5-a387-259fd851313e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
